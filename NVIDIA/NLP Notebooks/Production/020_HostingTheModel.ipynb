{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Hosting the Model\n",
    "\n",
    "In this notebook, you'll learn strategies to optimize Triton Server to improve the performance of your deployment.\n",
    "\n",
    "\n",
    "**[2.1 Concurrent Model Execution](#2.1-Concurrent-Model-Execution)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.1 Exercise: Usage Considerations](#2.1.1-Exercise:-Usage-Considerations)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.2 Implementation](#2.1.2-Implementation)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.3 Exercise: Configure Multiple Instance Groups](#2.1.3-Exercise:-Configure-Multiple-Instance-Groups)<br>\n",
    "**[2.2 Scheduling Strategies](#2.2-Scheduling-Strategies)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.1 Stateless Inference](#2.2.1-Stateless-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.2 Stateful Inference](#2.2.2-Stateful-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.3 Pipelines / Ensembles](#2.2.3-Pipelines-/-Ensembles)<br>\n",
    "**[2.3 Dynamic Batching](#2.3-Dynamic-Batching)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.3.1 Exercise: Implement Dynamic Batching](#2.3.1-Exercise:-Implement-Dynamic-Batching)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've executed customer requests sequentially, in the order they have arrived at the server, and used a static batch of size 8 for any requests to our server. This has not only left our GPUs heavily underutilized, but has also significantly affected the latency of responses received from the server. This is not an uncommon situation. Unless you are developing an application that processes large volumes of data in batch, you will likely be sending individual inference requests from the user application, leading to even further underutilization. As we have seen in the previous notebook, model optimizations do help considerably to accelerate model execution.  However, they do not change the fact that when serving is implemented naively, the nature of the inference workload leads to GPU underutilization.\n",
    "\n",
    "Inference servers, such as NVIDIA Triton, implement a wide range of features that allow us to improve the GPU utilization and improve request latency. The three that we will discuss in this class are:<br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/architecture.html#section-concurrent-model-execution\">Concurrent model execution</a></br>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/models_and_schedulers.html\">Scheduling</a> <br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/model_configuration.html#section-dynamic-batcher\">Dynamic batching</a> <br/>\n",
    "\n",
    "\n",
    "Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/quickstart.html\">Triton documentation</a> and its <a href=\"https://github.com/NVIDIA/triton-inference-server\">source code</a> for further information about the mechanisms and configurations that can help improve model inference performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Concurrent Model Execution\n",
    "The Triton architecture allows multiple models and/or multiple instances of the same model to execute in parallel on a single GPU. The following figure shows an example with two models: `model0` and `model1`. Assuming Triton is not currently processing any request, when two requests arrive simultaneously, one for each model, Triton immediately schedules both of them onto the GPU, and the GPUâ€™s hardware scheduler begins working on both computations in parallel. </br>\n",
    "\n",
    "<img src=\"images/multi_model_exec.png\"/><br/>\n",
    "\n",
    "#### Default Behavior\n",
    "\n",
    "By default, if multiple requests for the same model arrive at the same time, Triton will serialize their execution by scheduling only one at a time on the GPU, as shown in the following figure.\n",
    "\n",
    "<img src=\"images/multi_model_serial_exec.png\"/><br/>\n",
    "\n",
    "Triton provides an instance-group feature that allows each model to specify how many parallel executions of that model should be allowed. Each such enabled parallel execution is referred to as an *execution instance*. By default, Triton gives each model a single execution instance, which means that only a single execution of the model is allowed to be in progress at a time as shown in the above figure. \n",
    "\n",
    "#### Instance Groups\n",
    "By using the *instance-group* setting, the number of execution instances for a model can be increased. The following figure shows model execution when `model1` is configured to allow three execution instances. As shown in the figure, the first three `model1` inference requests are immediately executed in parallel on the GPU. The fourth `model1` inference request must wait until one of the first three executions completes before beginning.\n",
    "\n",
    "<img src=\"images/multi_model_parallel_exec.png\"/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Exercise: Usage Considerations\n",
    "\n",
    "For most models, the Triton feature that provides the largest performance improvement is *dynamic batching*. The key advantages of dynamic batching over setting up multiple instance execution are:\n",
    "- No overhead for model parameter storage\n",
    "- No overhead related to model parameter fetch from the GPU memory\n",
    "- Better utilization of the GPU resources\n",
    "\n",
    "Before we look at the configuration for multiple model execution, let's execute our model again using a single instance, and observe the resource utilization of the GPU. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Steps\n",
    "1. Launch a terminal window from the JupyterLab launch page.  If you need to open a new launch page, click the '+' icon on the left sidebar menu. You can then use a drag-and-drop action to move the terminal to a sub-window configuration  for better viewing.\n",
    "2. Execute the following command in the terminal before you run the performance tool:<br>\n",
    "\n",
    "```\n",
    "watch -n0.5 nvidia-smi\n",
    "```\n",
    "    You should see an output that resembles:\n",
    "<img src=\"images/NVIDIASMI.png\" style=\"position:relative; left:30px;\" width=800/>\n",
    "\n",
    "3. Execute the same benchmark we used in the previous notebook, but with the batch size reduced to 1, and observe the <code>nvidia-smi</code> output again.  Pay special attention to the memory consumption and GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previous configuration.\n",
    "modelVersion=\"1\"\n",
    "precision=\"fp32\"\n",
    "batchSize=\"1\"\n",
    "maxLatency=\"500\"\n",
    "maxClientThreads=\"10\"\n",
    "maxConcurrency=\"2\"\n",
    "dockerBridge=\"host\"\n",
    "resultsFolderName=\"1\"\n",
    "profilingData=\"utilities/profiling_data_int64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 36.3333 infer/sec. Avg latency: 27508 usec (std 167 usec)\n",
      "  Pass [2] throughput: 36 infer/sec. Avg latency: 27664 usec (std 166 usec)\n",
      "  Pass [3] throughput: 36.6667 infer/sec. Avg latency: 27481 usec (std 59 usec)\n",
      "  Client: \n",
      "    Request count: 110\n",
      "    Throughput: 36.6667 infer/sec\n",
      "    Avg latency: 27481 usec (standard deviation 59 usec)\n",
      "    p50 latency: 27471 usec\n",
      "    p90 latency: 27527 usec\n",
      "    p95 latency: 27559 usec\n",
      "    p99 latency: 27635 usec\n",
      "    Avg HTTP time: 27471 usec (send 5 usec + response wait 27465 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 131\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 27168 usec (overhead 3 usec + queue 28 usec + compute input 11 usec + compute infer 27115 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 37 infer/sec. Avg latency: 54517 usec (std 219 usec)\n",
      "  Pass [2] throughput: 36.6667 infer/sec. Avg latency: 54555 usec (std 236 usec)\n",
      "  Pass [3] throughput: 36.6667 infer/sec. Avg latency: 54504 usec (std 209 usec)\n",
      "  Client: \n",
      "    Request count: 110\n",
      "    Throughput: 36.6667 infer/sec\n",
      "    Avg latency: 54504 usec (standard deviation 209 usec)\n",
      "    p50 latency: 54410 usec\n",
      "    p90 latency: 54905 usec\n",
      "    p95 latency: 54993 usec\n",
      "    p99 latency: 55107 usec\n",
      "    Avg HTTP time: 54500 usec (send 5 usec + response wait 54494 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 132\n",
      "    Execution count: 132\n",
      "    Successful request count: 132\n",
      "    Avg request latency: 54180 usec (overhead 3 usec + queue 27003 usec + compute input 9 usec + compute infer 27154 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 36.6667 infer/sec. Avg latency: 82004 usec (std 366 usec)\n",
      "  Pass [2] throughput: 36.6667 infer/sec. Avg latency: 82084 usec (std 462 usec)\n",
      "  Pass [3] throughput: 36.3333 infer/sec. Avg latency: 82073 usec (std 389 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 36.3333 infer/sec\n",
      "    Avg latency: 82073 usec (standard deviation 389 usec)\n",
      "    p50 latency: 81956 usec\n",
      "    p90 latency: 82610 usec\n",
      "    p95 latency: 82712 usec\n",
      "    p99 latency: 82892 usec\n",
      "    Avg HTTP time: 82063 usec (send 6 usec + response wait 82056 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 132\n",
      "    Execution count: 132\n",
      "    Successful request count: 132\n",
      "    Avg request latency: 81698 usec (overhead 3 usec + queue 54417 usec + compute input 9 usec + compute infer 27257 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 36.6667 infer/sec. Avg latency: 109563 usec (std 512 usec)\n",
      "  Pass [2] throughput: 36.6667 infer/sec. Avg latency: 109583 usec (std 474 usec)\n",
      "  Pass [3] throughput: 36.3333 infer/sec. Avg latency: 109529 usec (std 473 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 36.3333 infer/sec\n",
      "    Avg latency: 109529 usec (standard deviation 473 usec)\n",
      "    p50 latency: 109396 usec\n",
      "    p90 latency: 110274 usec\n",
      "    p95 latency: 110406 usec\n",
      "    p99 latency: 110498 usec\n",
      "    Avg HTTP time: 109546 usec (send 6 usec + response wait 109539 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 131\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 109198 usec (overhead 3 usec + queue 81890 usec + compute input 9 usec + compute infer 27285 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 36.6667 infer/sec. Avg latency: 137135 usec (std 510 usec)\n",
      "  Pass [2] throughput: 36.6667 infer/sec. Avg latency: 137332 usec (std 1141 usec)\n",
      "  Pass [3] throughput: 36.6667 infer/sec. Avg latency: 137174 usec (std 542 usec)\n",
      "  Client: \n",
      "    Request count: 110\n",
      "    Throughput: 36.6667 infer/sec\n",
      "    Avg latency: 137174 usec (standard deviation 542 usec)\n",
      "    p50 latency: 137156 usec\n",
      "    p90 latency: 137935 usec\n",
      "    p95 latency: 138016 usec\n",
      "    p99 latency: 138129 usec\n",
      "    Avg HTTP time: 137135 usec (send 7 usec + response wait 137126 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 131\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 136757 usec (overhead 3 usec + queue 109408 usec + compute input 10 usec + compute infer 27324 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 36.6667 infer/sec. Avg latency: 164747 usec (std 507 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 164858 usec (std 470 usec)\n",
      "  Pass [3] throughput: 36 infer/sec. Avg latency: 165739 usec (std 905 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 36 infer/sec\n",
      "    Avg latency: 165739 usec (standard deviation 905 usec)\n",
      "    p50 latency: 165633 usec\n",
      "    p90 latency: 166927 usec\n",
      "    p95 latency: 167333 usec\n",
      "    p99 latency: 167618 usec\n",
      "    Avg HTTP time: 165844 usec (send 9 usec + response wait 165833 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 130\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 165408 usec (overhead 4 usec + queue 137861 usec + compute input 13 usec + compute infer 27513 usec + compute output 17 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 36.3333 infer/sec. Avg latency: 193501 usec (std 809 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 193160 usec (std 660 usec)\n",
      "  Pass [3] throughput: 36 infer/sec. Avg latency: 193453 usec (std 600 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 36 infer/sec\n",
      "    Avg latency: 193453 usec (standard deviation 600 usec)\n",
      "    p50 latency: 193491 usec\n",
      "    p90 latency: 194159 usec\n",
      "    p95 latency: 194455 usec\n",
      "    p99 latency: 194724 usec\n",
      "    Avg HTTP time: 193428 usec (send 9 usec + response wait 193417 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 131\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 193037 usec (overhead 3 usec + queue 165491 usec + compute input 11 usec + compute infer 27519 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 36.3333 infer/sec. Avg latency: 221179 usec (std 563 usec)\n",
      "  Pass [2] throughput: 36 infer/sec. Avg latency: 221429 usec (std 689 usec)\n",
      "  Pass [3] throughput: 36 infer/sec. Avg latency: 221388 usec (std 477 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 36 infer/sec\n",
      "    Avg latency: 221388 usec (standard deviation 477 usec)\n",
      "    p50 latency: 221414 usec\n",
      "    p90 latency: 221886 usec\n",
      "    p95 latency: 222302 usec\n",
      "    p99 latency: 222687 usec\n",
      "    Avg HTTP time: 221461 usec (send 8 usec + response wait 221451 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 130\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 221076 usec (overhead 2 usec + queue 193474 usec + compute input 11 usec + compute infer 27577 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 36 infer/sec. Avg latency: 249289 usec (std 2929 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 249700 usec (std 634 usec)\n",
      "  Pass [3] throughput: 36 infer/sec. Avg latency: 249870 usec (std 611 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 36 infer/sec\n",
      "    Avg latency: 249870 usec (standard deviation 611 usec)\n",
      "    p50 latency: 249724 usec\n",
      "    p90 latency: 250959 usec\n",
      "    p95 latency: 251111 usec\n",
      "    p99 latency: 251208 usec\n",
      "    Avg HTTP time: 249952 usec (send 10 usec + response wait 249940 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 129\n",
      "    Execution count: 129\n",
      "    Successful request count: 129\n",
      "    Avg request latency: 249543 usec (overhead 3 usec + queue 221860 usec + compute input 12 usec + compute infer 27654 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 35.6667 infer/sec. Avg latency: 277332 usec (std 4138 usec)\n",
      "  Pass [2] throughput: 35.6667 infer/sec. Avg latency: 277824 usec (std 624 usec)\n",
      "  Pass [3] throughput: 36 infer/sec. Avg latency: 277956 usec (std 475 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 36 infer/sec\n",
      "    Avg latency: 277956 usec (standard deviation 475 usec)\n",
      "    p50 latency: 277958 usec\n",
      "    p90 latency: 278594 usec\n",
      "    p95 latency: 278719 usec\n",
      "    p99 latency: 278915 usec\n",
      "    Avg HTTP time: 278118 usec (send 8 usec + response wait 278108 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 130\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 277738 usec (overhead 3 usec + queue 250005 usec + compute input 10 usec + compute infer 27707 usec + compute output 13 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 36.6667 infer/sec, latency 27481 usec\n",
      "Concurrency: 2, throughput: 36.6667 infer/sec, latency 54504 usec\n",
      "Concurrency: 3, throughput: 36.3333 infer/sec, latency 82073 usec\n",
      "Concurrency: 4, throughput: 36.3333 infer/sec, latency 109529 usec\n",
      "Concurrency: 5, throughput: 36.6667 infer/sec, latency 137174 usec\n",
      "Concurrency: 6, throughput: 36 infer/sec, latency 165739 usec\n",
      "Concurrency: 7, throughput: 36 infer/sec, latency 193453 usec\n",
      "Concurrency: 8, throughput: 36 infer/sec, latency 221388 usec\n",
      "Concurrency: 9, throughput: 36 infer/sec, latency 249870 usec\n",
      "Concurrency: 10, throughput: 36 infer/sec, latency 277956 usec\n"
     ]
    }
   ],
   "source": [
    "# Update configuration parameters and run profiler.\n",
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \" + modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you have observed utilization similar to the following:<br/>\n",
    "<img src=\"images/NVIDIASMI2.png\" width=800/><br/>\n",
    "\n",
    "Do you think you will observe a major acceleration as a consequence of increasing the number of instance groups?<br>\n",
    "Discuss with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Implementation\n",
    "Let's look at how to enable concurrent execution and what impact it will have on our model performance. Execute the following code cells to export the model in the ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-conexec\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-conexec in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.4295821189880371 seconds\n",
      "time of error check of onnx model:  20.647558450698853 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-xr-x 3 root root 4.0K Oct  6 08:09 .\n",
      "drwxr-xr-x 3 root root 4.0K Oct  6 08:08 ..\n",
      "drwxr-xr-x 2 root root 4.0K Oct  6 08:08 1\n",
      "-rw-r--r-- 1 root root  569 Oct  6 08:09 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./candidatemodels/bertQA-onnx-conexec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 Exercise: Configure Multiple Instance Groups\n",
    "In order to specify multiple instances, we need to change the \"count\" value from '1' to a larger number in the `instance_group` section of the \"config.pbtxt\" configuration file. \n",
    "\n",
    "\n",
    "```\n",
    "    instance_group [\n",
    "    {\n",
    "        count: 2\n",
    "        kind: KIND_GPU\n",
    "        gpus: [ 0 ]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "#### Exercise Steps:\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-conexec/config.pbtxt) in the `bertQA-onnx-conexec` deployment just created to specify two instances of our BERT-based question answering model. You should find the default instance_group block at the end of the file. Change the count variable from 1 to 2.  (see the [solution](solutions/ex-2-1-3_config.pbtxt) as needed)\n",
    "2. To make the comparison fair, also enable TensorRT with the addition of an `execution_accelerators` block inside the `optimization` block:\n",
    "\n",
    "```text\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "```\n",
    "\n",
    "3. Once you have saved your changes (Main menu: File -> Save File), move the model across to Triton by executing the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: inter-device move failed: './candidatemodels/bertQA-onnx-conexec' to 'model_repository/bertQA-onnx-conexec'; unable to remove target: Directory not empty\n"
     ]
    }
   ],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-conexec model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run our standard stress test against the model. Please compare it to the single instance execution.<br>\n",
    "   Did the throughput change?<br>\n",
    "   Did the latency change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-conexec\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 36 infer/sec. Avg latency: 27690 usec (std 325 usec)\n",
      "  Pass [2] throughput: 36 infer/sec. Avg latency: 27653 usec (std 116 usec)\n",
      "  Pass [3] throughput: 36.3333 infer/sec. Avg latency: 27687 usec (std 334 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 36.3333 infer/sec\n",
      "    Avg latency: 27687 usec (standard deviation 334 usec)\n",
      "    p50 latency: 27639 usec\n",
      "    p90 latency: 27826 usec\n",
      "    p95 latency: 27907 usec\n",
      "    p99 latency: 28152 usec\n",
      "    Avg HTTP time: 27658 usec (send 6 usec + response wait 27650 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 130\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 27284 usec (overhead 3 usec + queue 27 usec + compute input 12 usec + compute infer 27229 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 36.6667 infer/sec. Avg latency: 54941 usec (std 357 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 54965 usec (std 330 usec)\n",
      "  Pass [3] throughput: 36.3333 infer/sec. Avg latency: 55008 usec (std 296 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 36.3333 infer/sec\n",
      "    Avg latency: 55008 usec (standard deviation 296 usec)\n",
      "    p50 latency: 54953 usec\n",
      "    p90 latency: 55409 usec\n",
      "    p95 latency: 55474 usec\n",
      "    p99 latency: 55681 usec\n",
      "    Avg HTTP time: 55012 usec (send 9 usec + response wait 55001 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 131\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 54632 usec (overhead 4 usec + queue 27212 usec + compute input 11 usec + compute infer 27389 usec + compute output 16 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 36 infer/sec. Avg latency: 82625 usec (std 472 usec)\n",
      "  Pass [2] throughput: 36.6667 infer/sec. Avg latency: 82440 usec (std 347 usec)\n",
      "  Pass [3] throughput: 36.3333 infer/sec. Avg latency: 82536 usec (std 445 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 36.3333 infer/sec\n",
      "    Avg latency: 82536 usec (standard deviation 445 usec)\n",
      "    p50 latency: 82418 usec\n",
      "    p90 latency: 83116 usec\n",
      "    p95 latency: 83217 usec\n",
      "    p99 latency: 83809 usec\n",
      "    Avg HTTP time: 82524 usec (send 9 usec + response wait 82513 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 131\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 82104 usec (overhead 3 usec + queue 54686 usec + compute input 12 usec + compute infer 27388 usec + compute output 15 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 36 infer/sec. Avg latency: 110209 usec (std 455 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 110226 usec (std 568 usec)\n",
      "  Pass [3] throughput: 36 infer/sec. Avg latency: 110436 usec (std 648 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 36 infer/sec\n",
      "    Avg latency: 110436 usec (standard deviation 648 usec)\n",
      "    p50 latency: 110329 usec\n",
      "    p90 latency: 111184 usec\n",
      "    p95 latency: 111790 usec\n",
      "    p99 latency: 112443 usec\n",
      "    Avg HTTP time: 110513 usec (send 10 usec + response wait 110501 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 130\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 110056 usec (overhead 3 usec + queue 82524 usec + compute input 14 usec + compute infer 27495 usec + compute output 20 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 36 infer/sec. Avg latency: 138678 usec (std 1067 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 138198 usec (std 708 usec)\n",
      "  Pass [3] throughput: 36.3333 infer/sec. Avg latency: 138212 usec (std 620 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 36.3333 infer/sec\n",
      "    Avg latency: 138212 usec (standard deviation 620 usec)\n",
      "    p50 latency: 138187 usec\n",
      "    p90 latency: 139025 usec\n",
      "    p95 latency: 139271 usec\n",
      "    p99 latency: 139662 usec\n",
      "    Avg HTTP time: 138204 usec (send 12 usec + response wait 138190 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 130\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 137770 usec (overhead 2 usec + queue 110226 usec + compute input 13 usec + compute infer 27512 usec + compute output 17 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 36 infer/sec. Avg latency: 165899 usec (std 466 usec)\n",
      "  Pass [2] throughput: 36 infer/sec. Avg latency: 165921 usec (std 466 usec)\n",
      "  Pass [3] throughput: 36 infer/sec. Avg latency: 166081 usec (std 407 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 36 infer/sec\n",
      "    Avg latency: 166081 usec (standard deviation 407 usec)\n",
      "    p50 latency: 166113 usec\n",
      "    p90 latency: 166606 usec\n",
      "    p95 latency: 166829 usec\n",
      "    p99 latency: 166924 usec\n",
      "    Avg HTTP time: 166051 usec (send 10 usec + response wait 166039 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 130\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 165623 usec (overhead 2 usec + queue 138041 usec + compute input 13 usec + compute infer 27552 usec + compute output 15 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 36 infer/sec. Avg latency: 194054 usec (std 671 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 193495 usec (std 613 usec)\n",
      "  Pass [3] throughput: 36 infer/sec. Avg latency: 194354 usec (std 779 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 36 infer/sec\n",
      "    Avg latency: 194354 usec (standard deviation 779 usec)\n",
      "    p50 latency: 194148 usec\n",
      "    p90 latency: 195447 usec\n",
      "    p95 latency: 195939 usec\n",
      "    p99 latency: 196095 usec\n",
      "    Avg HTTP time: 194256 usec (send 11 usec + response wait 194243 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 130\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 193791 usec (overhead 3 usec + queue 166149 usec + compute input 14 usec + compute infer 27605 usec + compute output 20 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 36.3333 infer/sec. Avg latency: 221624 usec (std 834 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 221978 usec (std 645 usec)\n",
      "  Pass [3] throughput: 36 infer/sec. Avg latency: 222421 usec (std 705 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 36 infer/sec\n",
      "    Avg latency: 222421 usec (standard deviation 705 usec)\n",
      "    p50 latency: 222321 usec\n",
      "    p90 latency: 223339 usec\n",
      "    p95 latency: 223898 usec\n",
      "    p99 latency: 224170 usec\n",
      "    Avg HTTP time: 222378 usec (send 10 usec + response wait 222366 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 130\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 221951 usec (overhead 3 usec + queue 194244 usec + compute input 13 usec + compute infer 27676 usec + compute output 15 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 35.6667 infer/sec. Avg latency: 250238 usec (std 3061 usec)\n",
      "  Pass [2] throughput: 36 infer/sec. Avg latency: 250801 usec (std 844 usec)\n",
      "  Pass [3] throughput: 35.6667 infer/sec. Avg latency: 250671 usec (std 633 usec)\n",
      "  Client: \n",
      "    Request count: 107\n",
      "    Throughput: 35.6667 infer/sec\n",
      "    Avg latency: 250671 usec (standard deviation 633 usec)\n",
      "    p50 latency: 250446 usec\n",
      "    p90 latency: 251429 usec\n",
      "    p95 latency: 251906 usec\n",
      "    p99 latency: 252325 usec\n",
      "    Avg HTTP time: 250628 usec (send 10 usec + response wait 250616 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 130\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 250207 usec (overhead 3 usec + queue 222441 usec + compute input 13 usec + compute infer 27735 usec + compute output 15 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 36 infer/sec. Avg latency: 277777 usec (std 4434 usec)\n",
      "  Pass [2] throughput: 35.6667 infer/sec. Avg latency: 279721 usec (std 1061 usec)\n",
      "  Pass [3] throughput: 35.6667 infer/sec. Avg latency: 279387 usec (std 856 usec)\n",
      "  Client: \n",
      "    Request count: 107\n",
      "    Throughput: 35.6667 infer/sec\n",
      "    Avg latency: 279387 usec (standard deviation 856 usec)\n",
      "    p50 latency: 279198 usec\n",
      "    p90 latency: 280302 usec\n",
      "    p95 latency: 281735 usec\n",
      "    p99 latency: 282068 usec\n",
      "    Avg HTTP time: 279379 usec (send 10 usec + response wait 279367 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 129\n",
      "    Execution count: 129\n",
      "    Successful request count: 129\n",
      "    Avg request latency: 278931 usec (overhead 3 usec + queue 251104 usec + compute input 13 usec + compute infer 27793 usec + compute output 18 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 36.3333 infer/sec, latency 27687 usec\n",
      "Concurrency: 2, throughput: 36.3333 infer/sec, latency 55008 usec\n",
      "Concurrency: 3, throughput: 36.3333 infer/sec, latency 82536 usec\n",
      "Concurrency: 4, throughput: 36 infer/sec, latency 110436 usec\n",
      "Concurrency: 5, throughput: 36.3333 infer/sec, latency 138212 usec\n",
      "Concurrency: 6, throughput: 36 infer/sec, latency 166081 usec\n",
      "Concurrency: 7, throughput: 36 infer/sec, latency 194354 usec\n",
      "Concurrency: 8, throughput: 36 infer/sec, latency 222421 usec\n",
      "Concurrency: 9, throughput: 35.6667 infer/sec, latency 250671 usec\n",
      "Concurrency: 10, throughput: 35.6667 infer/sec, latency 279387 usec\n"
     ]
    }
   ],
   "source": [
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \" + modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's free up some GPU memory by moving some of the models out of the Triton model repository.  After removing the following three models, only the `bertQA-torchscript` model should remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: inter-device move failed: '/dli/task/model_repository/bertQA-onnx-conexec' to '/dli/task/candidatemodels/bertQA-onnx-conexec'; unable to remove target: Directory not empty\n",
      "bertQA-onnx-conexec  bertQA-torchscript\n"
     ]
    }
   ],
   "source": [
    "# Remove models from the inference server by removing them from the model_repository\n",
    "!mv /dli/task/model_repository/bertQA-onnx /dli/task/candidatemodels/\n",
    "!mv /dli/task/model_repository/bertQA-onnx-conexec /dli/task/candidatemodels/\n",
    "!mv /dli/task/model_repository/bertQA-onnx-trt-fp16 /dli/task/candidatemodels/\n",
    "\n",
    "# List remaining models on the inference server\n",
    "!ls /dli/task/model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Scheduling Strategies\n",
    "Triton supports batch inferencing by allowing individual inference requests to specify a batch of inputs. The inferencing for a batch of inputs is performed at the same time which is especially important for GPUs since it can greatly increase inferencing throughput. In many use cases the individual inference requests are not batched, therefore, they do not benefit from the throughput benefits of batching. <br/>\n",
    "\n",
    "The inference server contains multiple scheduling and batching algorithms that support many different model types and use-cases. The choice of the scheduler / batcher will be driven by several factors the key ones being:\n",
    "- Stateful / stateless nature of your inference workload\n",
    "- Whether your application is composed of models served in isolation or whether a more complex pipeline / ensemble is being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Stateless Inference\n",
    "\n",
    "When dealing with stateless inference (as we are in this class) we have two main options when it comes to scheduling. The first option is the default scheduler which will distribute request to all instances assigned for inference. This is the preferred option when the structure of the inference workload is well understood and where inference will take place at regular batch sizes and time intervals.\n",
    "\n",
    "The second option is dynamic batching which combines individual request and similarly to the default batcher distributes the larges batches across instances. We will discuss this particular option in the next section of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Stateful Inference\n",
    "\n",
    "A stateful model (or stateful custom backend) does maintain state between inference requests. The model is expecting multiple inference requests that together form a sequence of inferences that must be routed to the same model instance so that the state being maintained by the model is correctly updated. Moreover, the model may require that Triton provide control signals indicating, for example, sequence start.\n",
    "\n",
    "The sequence batcher can employ one of two scheduling strategies when deciding how to batch the sequences that are routed to the same model instance. These strategies are Direct and Oldest.\n",
    "\n",
    "With the Direct scheduling strategy the sequence batcher ensures not only that all inference requests in a sequence are routed to the same model instance, but also that each sequence is routed to a dedicated batch slot within the model instance. This strategy is required when the model maintains state for each batch slot, and is expecting all inference requests for a given sequence to be routed to the same slot so that the state is correctly updated.\n",
    "\n",
    "With the Oldest scheduling strategy the sequence batcher ensures that all inference requests in a sequence are routed to the same model instance and then uses the dynamic batcher to batch together multiple inferences from different sequences into a batch that inferences together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Pipelines / Ensembles\n",
    "\n",
    "An ensemble model represents a pipeline of one or more models and the connection of input and output tensors between those models. Ensemble models are intended to be used to encapsulate a procedure that involves multiple models, such as \"data preprocessing -> inference -> data post-processing\". Using ensemble models for this purpose can avoid the overhead of transferring intermediate tensors and minimize the number of requests that must be sent to Triton. An example of an ensemble pipeline is illustrated below: <br/>\n",
    "\n",
    "<img src=\"images/ensemble_example0.png\"/>\n",
    "\n",
    "The ensemble scheduler must be used for ensemble models, regardless of the scheduler used by the models within the ensemble. With respect to the ensemble scheduler, an ensemble model is not an actual model. Instead, it specifies the data flow between models within the ensemble as Step. The scheduler collects the output tensors in each step, provides them as input tensors for other steps according to the specification. In spite of that, the ensemble model is still viewed as a single model from an external view.\n",
    "\n",
    "More information on Triton scheduling can be found in the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/models_and_schedulers.html#stateless-models\">following section of the documentation</a>. In this class, we will focus further on one of the most powerful features of Triton, *dynamic batching*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Dynamic Batching\n",
    "Dynamic batching is a feature of Triton that allows inference requests to be combined by the server, so that a batch is created dynamically, resulting in increased throughput.\n",
    "\n",
    "When a model instance becomes available for inferencing, the dynamic batcher will attempt to create batches from the requests that are available in the scheduler. Requests are added to the batch in the order the requests were received. If the dynamic batcher can form a batch of a preferred size(s) it will create a batch of the largest possible preferred size and send it for inferencing. If the dynamic batcher cannot form a batch of a preferred size, it will send a batch of the largest size possible that is less than the max batch size allowed by the model. \n",
    "\n",
    "The dynamic batcher can be configured to allow requests to be delayed for a limited time in the scheduler to allow other requests to join the dynamic batch. For example, the following configuration sets the maximum delay time of 100 microseconds for a request:\n",
    "\n",
    "```\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 4, 8 ]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Exercise: Implement Dynamic Batching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin again by exporting an ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-trt-dynbatch in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.4185009002685547 seconds\n",
      "time of error check of onnx model:  18.951934814453125 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Steps\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-trt-dynbatch/config.pbtxt) for dynamic batching using the example snippet. \n",
    "\n",
    "    ```\n",
    "    dynamic_batching {\n",
    "      preferred_batch_size: [ 4, 8 ]\n",
    "      max_queue_delay_microseconds: 100\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "2. Enable TensorRT in the optimization block.\n",
    "\n",
    "    ```\n",
    "    optimization {\n",
    "       execution_accelerators {\n",
    "          gpu_execution_accelerator : [ {\n",
    "             name : \"tensorrt\"\n",
    "             parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "          }]\n",
    "       }\n",
    "    cuda { graphs: 0 }\n",
    "    }\n",
    "    ```\n",
    "3. Once saved, move the model to the Triton model repository and run the performance utility by executing the following cells. ([solution](solutions/ex-2-3-1_config.pbtxt) if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-trt-dynbatch model_repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-dynbatch\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "..........................Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 35.3333 infer/sec. Avg latency: 28269 usec (std 465 usec)\n",
      "  Pass [2] throughput: 35.3333 infer/sec. Avg latency: 28351 usec (std 358 usec)\n",
      "  Pass [3] throughput: 35.3333 infer/sec. Avg latency: 28315 usec (std 354 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 35.3333 infer/sec\n",
      "    Avg latency: 28315 usec (standard deviation 354 usec)\n",
      "    p50 latency: 28464 usec\n",
      "    p90 latency: 28710 usec\n",
      "    p95 latency: 28806 usec\n",
      "    p99 latency: 28924 usec\n",
      "    Avg HTTP time: 28280 usec (send 10 usec + response wait 28268 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 127\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 27766 usec (overhead 4 usec + queue 185 usec + compute input 19 usec + compute infer 27513 usec + compute output 45 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 36 infer/sec. Avg latency: 55571 usec (std 615 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 55526 usec (std 661 usec)\n",
      "  Pass [3] throughput: 36 infer/sec. Avg latency: 55503 usec (std 547 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 36 infer/sec\n",
      "    Avg latency: 55503 usec (standard deviation 547 usec)\n",
      "    p50 latency: 55637 usec\n",
      "    p90 latency: 56141 usec\n",
      "    p95 latency: 56350 usec\n",
      "    p99 latency: 56411 usec\n",
      "    Avg HTTP time: 55540 usec (send 10 usec + response wait 55528 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 130\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 55037 usec (overhead 4 usec + queue 27415 usec + compute input 17 usec + compute infer 27556 usec + compute output 45 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 53.3333 infer/sec. Avg latency: 55831 usec (std 584 usec)\n",
      "  Pass [2] throughput: 53.6667 infer/sec. Avg latency: 56009 usec (std 537 usec)\n",
      "  Pass [3] throughput: 53.3333 infer/sec. Avg latency: 55933 usec (std 571 usec)\n",
      "  Client: \n",
      "    Request count: 160\n",
      "    Throughput: 53.3333 infer/sec\n",
      "    Avg latency: 55933 usec (standard deviation 571 usec)\n",
      "    p50 latency: 56075 usec\n",
      "    p90 latency: 56573 usec\n",
      "    p95 latency: 56723 usec\n",
      "    p99 latency: 56865 usec\n",
      "    Avg HTTP time: 55872 usec (send 11 usec + response wait 55859 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 193\n",
      "    Execution count: 129\n",
      "    Successful request count: 129\n",
      "    Avg request latency: 55283 usec (overhead 5 usec + queue 27533 usec + compute input 23 usec + compute infer 27654 usec + compute output 68 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 71.3333 infer/sec. Avg latency: 56006 usec (std 580 usec)\n",
      "  Pass [2] throughput: 71.3333 infer/sec. Avg latency: 55845 usec (std 625 usec)\n",
      "  Pass [3] throughput: 71.3333 infer/sec. Avg latency: 55932 usec (std 659 usec)\n",
      "  Client: \n",
      "    Request count: 214\n",
      "    Throughput: 71.3333 infer/sec\n",
      "    Avg latency: 55932 usec (standard deviation 659 usec)\n",
      "    p50 latency: 56229 usec\n",
      "    p90 latency: 56759 usec\n",
      "    p95 latency: 56828 usec\n",
      "    p99 latency: 56955 usec\n",
      "    Avg HTTP time: 55950 usec (send 10 usec + response wait 55938 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 258\n",
      "    Execution count: 129\n",
      "    Successful request count: 129\n",
      "    Avg request latency: 55338 usec (overhead 4 usec + queue 27557 usec + compute input 22 usec + compute infer 27669 usec + compute output 86 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 89.3333 infer/sec. Avg latency: 56156 usec (std 655 usec)\n",
      "  Pass [2] throughput: 88.3333 infer/sec. Avg latency: 56347 usec (std 545 usec)\n",
      "  Pass [3] throughput: 88.3333 infer/sec. Avg latency: 56335 usec (std 653 usec)\n",
      "  Client: \n",
      "    Request count: 265\n",
      "    Throughput: 88.3333 infer/sec\n",
      "    Avg latency: 56335 usec (standard deviation 653 usec)\n",
      "    p50 latency: 56487 usec\n",
      "    p90 latency: 57134 usec\n",
      "    p95 latency: 57262 usec\n",
      "    p99 latency: 57389 usec\n",
      "    Avg HTTP time: 56285 usec (send 11 usec + response wait 56272 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 320\n",
      "    Execution count: 128\n",
      "    Successful request count: 128\n",
      "    Avg request latency: 55602 usec (overhead 5 usec + queue 27688 usec + compute input 27 usec + compute infer 27765 usec + compute output 117 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 106 infer/sec. Avg latency: 56235 usec (std 690 usec)\n",
      "  Pass [2] throughput: 106 infer/sec. Avg latency: 56163 usec (std 676 usec)\n",
      "  Pass [3] throughput: 107.333 infer/sec. Avg latency: 56465 usec (std 575 usec)\n",
      "  Client: \n",
      "    Request count: 322\n",
      "    Throughput: 107.333 infer/sec\n",
      "    Avg latency: 56465 usec (standard deviation 575 usec)\n",
      "    p50 latency: 56676 usec\n",
      "    p90 latency: 57050 usec\n",
      "    p95 latency: 57205 usec\n",
      "    p99 latency: 57362 usec\n",
      "    Avg HTTP time: 56403 usec (send 12 usec + response wait 56389 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 380\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 55669 usec (overhead 4 usec + queue 27697 usec + compute input 30 usec + compute infer 27801 usec + compute output 137 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 123.667 infer/sec. Avg latency: 56624 usec (std 583 usec)\n",
      "  Pass [2] throughput: 122.667 infer/sec. Avg latency: 56768 usec (std 1182 usec)\n"
     ]
    }
   ],
   "source": [
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "maxConcurency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \"+modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have observed a fairly dramatic improvement in both latency and throughput. \n",
    "* How big is the impact in comparison to vanilla ONNX configuration or vanilla TorchScript? \n",
    "* What do you think was bottlenecking the multiple instance implementation?\n",
    "\n",
    "Discuss the results with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Congratulations!</h3><br>\n",
    "You've leaned some strategies to improve the GPU utilization and reduce latency using:\n",
    "\n",
    "* Concurrent model execution\n",
    "* Scheduling\n",
    "* Dynamic batching\n",
    "\n",
    "In the next segment of the class we will make a more formal assessment of inference performance across multiple concurrency levels and how to analyze your inference performance in a structured way. Please proceed to the next notebook:<br>\n",
    "[3.0 Server Performance](030_ServerPerformance.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
